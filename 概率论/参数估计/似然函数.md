---
tags:
  - 数学
dlink:
  - "[[最大似然估计]]"
author:
  - Cyletix
  - GPT-4
aliases:
  - Likelihood
  - 尤度
finished: false
---
在概率和统计学中，尤度是一个函数，在给定参数下，表示观测数据出现的**概率**。与概率不同的是，尤度函数将参数视为变量，而观测数据是已知的。尤度函数通常用于参数估计，尤其是在[[最大似然估计]]中。

尤度函数是基于误差项（如正态分布的误差）的概率密度函数，而没有误差项时，模型变成了一个确定性的线性方程组，无法定义概率分布，也就没有尤度函数可言。在这种确定性模型中，我们可以通过直接解方程组来确定。如果线性方程组没有一致的解，这意味着数据本身存在矛盾或测量错误。



# 似然函数和对数似然函数

在统计学习和机器学习中，似然函数（Likelihood Function）和对数似然函数（Log-Likelihood Function）用于描述模型参数在给定数据上的表现。

---

## 1. 似然函数

对于给定的参数 $\theta$ 和数据集 $\{(x_i, y_i)\}_{i=1}^N$，假设数据是独立同分布的（IID），则似然函数表示参数 $\theta$ 下所有样本点联合概率的乘积：

$$
L(\theta; x, y) = \prod_{i=1}^{N} p(y_i \mid x_i, \theta)
$$

**解读**：  
- $p(y_i \mid x_i, \theta)$ 表示在参数 $\theta$ 下，模型预测 $x_i$ 的标签为 $y_i$ 的概率。
- 通过最大化这个似然函数，我们希望找到最能解释数据的模型参数 $\theta$。

---

## 2. 对数似然函数

直接计算似然函数的乘积容易导致数值下溢（即结果过小）。因此，我们对似然函数取对数，得到对数似然函数：

$$
\log L(\theta; x, y) = \sum_{i=1}^{N} \log p(y_i \mid x_i, \theta)
$$

**解读**：  
- 对数似然将乘法转化为加法，既简化了计算，也避免了数值问题。
- 对数似然值越大，模型的参数 $\theta$ 在给定数据上的解释力越强。

---

## 应用
### 二分类交叉熵
在二分类问题中，每个样本点的条件概率为：
$$
p(y_i \mid x_i, \theta) = [h_{\theta}(x_i)]^{y_i} [1 - h_{\theta}(x_i)]^{1 - y_i}
$$
将该概率代入似然函数：
$$
L(\theta; x, y) = \prod_{i=1}^{N} [h_{\theta}(x_i)]^{y_i} [1 - h_{\theta}(x_i)]^{1 - y_i}
$$
对该似然函数取对数：
$$
\log L(\theta; x, y) = \sum_{i=1}^{N} \left[ y_i \log h_{\theta}(x_i) + (1 - y_i) \log (1 - h_{\theta}(x_i)) \right]
$$
交叉熵损失是对数似然的相反数（即损失函数为负的对数似然），其公式为：

$$
J(\theta) = - \log L(\theta; x, y) = - \sum_{i=1}^{N} \left[ y_i \log h_{\theta}(x_i) + (1 - y_i) \log (1 - h_{\theta}(x_i)) \right]
$$

该公式用于衡量模型预测与实际标签之间的匹配程度。目标是最小化交叉熵损失，即最大化对数似然。


---
## 示例

给定一个模型 $y(a1, a2) = a1x1 + a2x2 + e$，其中 $e$ 服从标准正态分布 $N(0, 1)$，我们要根据三次观测值来估计 $x1$ 和 $x2$ 的尤度。

假设我们有三次观测值 $y_1, y_2, y_3$。这些观测值可以表示为：

$$
y_1 = a1x1 + a2x2 + e_1
$$
$$
y_2 = a1x1 + a2x2 + e_2
$$
$$
y_3 = a1x1 + a2x2 + e_3
$$

由于 $e_i$ 服从标准正态分布，因此每个 $y_i$ 的条件概率密度函数为：

$$
f(y_i | a1, a2, x1, x2) = \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{(y_i - (a1x1 + a2x2))^2}{2} \right)
$$

尤度函数 $L(a1, a2 | y_1, y_2, y_3, x1, x2)$ 是所有观测值的联合概率密度函数：

$$
L(a1, a2 | y_1, y_2, y_3, x1, x2) = \prod_{i=1}^3 f(y_i | a1, a2, x1, x2)
$$

将每个 $f(y_i | a1, a2, x1, x2)$ 代入，我们得到：

$$
L(a1, a2 | y_1, y_2, y_3, x1, x2) = \prod_{i=1}^3 \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{(y_i - (a1x1 + a2x2))^2}{2} \right)
$$

简化得到：

$$
L(a1, a2 | y_1, y_2, y_3, x1, x2) = \left( \frac{1}{\sqrt{2\pi}} \right)^3 \exp \left( -\frac{1}{2} \sum_{i=1}^3 (y_i - (a1x1 + a2x2))^2 \right)
$$

尤度函数的对数形式（对数尤度函数）通常更方便处理：

$$
\log L(a1, a2 | y_1, y_2, y_3, x1, x2) = -\frac{3}{2} \log(2\pi) - \frac{1}{2} \sum_{i=1}^3 (y_i - (a1x1 + a2x2))^2
$$

通过最大化对数尤度函数，我们可以估计 $a1$ 和 $a2$ 的最佳值，即最大似然估计（MLE）。实际估计过程中，我们会用到数值优化方法，如梯度下降或牛顿-拉夫森法。
